mean(EPI) #mean
mfv(EPI) #mode
median(EPI) #median
fivenum(EPI) # five num summary
DALY <- as.numeric(EPI_dataset$DALY)
DALY <- na.omit(DALY)
mean(DALY) #mean
mfv(DALY) #mode
median(DALY) #median
fivenum(DALY) # five num summary
#Generate frequency histograms for EPI and DALY
stem(EPI)
hist(EPI,seq(30,95,1.0),prob=TRUE)
#lines(density(EPI,na.rm=TRUE,bw=1))
lines(density(EPI,na.rm=TRUE,bw='SJ'))
stem(DALY)
hist(DALY,seq(0,100,1.0),prob=TRUE)
#lines(density(DALY,na.rm=TRUE,bw=1))
lines(density(DALY,na.rm=TRUE,bw='SJ'))
#Generate the boxplot
#?boxplot
boxplot(EPI_dataset$ENVHEALTH,EPI_dataset$ECOSYSTEM,names = c('ENVHEALTH', 'ECOSYSTEM'))
#Generate the Q-Q plot
#?qqplot
qqplot(EPI_dataset$ENVHEALTH,EPI_dataset$ECOSYSTEM,xlab = 'ENVHEALTH', ylab = 'ECOSYSTEM')
#-------------------------------------------------------
#                        Part 1b                        |
#-------------------------------------------------------
#Find the most important feature by region
table(EPI_dataset$EPI_regions)
EPI_SSA <- filter(EPI_dataset,EPI_regions == 'Sub-Saharan Africa')
EPI_SSA <- EPI_dataset[, unlist(lapply(EPI_SSA, is.numeric))]
EPI_SSA<-na.omit(EPI_SSA)
EPI_cor <- data.frame(cor(EPI_SSA,EPI_SSA$EPI))
EPI_cor <- na.omit(EPI_cor)
EPI_cor <- cbind(newColName = rownames(EPI_cor), EPI_cor)
rownames(EPI_cor) <- 1:nrow(EPI_cor)
colnames(EPI_cor) <- c('variable name','correlation_with_EPI')
EPI_cor <- EPI_cor %>% arrange(correlation_with_EPI)
View(EPI_cor)
#Linear and Least-Square
boxplot(EPI_dataset$ENVHEALTH,EPI_dataset$DALY,EPI_dataset$AIR_H,EPI_dataset$WATER_H,names = c('ENVHEALTH','DALY','AIR_H','WATER_H'))
lmENVH <- lm(ENVHEALTH~DALY+AIR_H+WATER_H, data = EPI_dataset)
lmENVH
summary(lmENVH)
cENVH <- coef(lmENVH)
lmENVH
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
colnames(NEW) <- c('DALY','AIR_H','WATER_H')
pENV <- lmENVH %>% predict(NEW, interval = 'prediction')
cENV <- lmENVH %>% predict(NEW, interval = 'confidence')
View(pENV)
#-------------------------------------------------------
#                        Part 2a                        |
#-------------------------------------------------------
#-------------------------------------------------------
#                        Part 2a                        |
#-------------------------------------------------------
reg_dataset <- read.csv('https://aquarius.tw.rpi.edu/html/DA/dataset_multipleRegression.csv')
reg_dataset
aba_dataset <- read.csv('https://aquarius.tw.rpi.edu/html/DA/abalone.csv')
lmfit <- reg_dataset %>% lm(UNEM ~ HGRAD + ROLL)
lmfit <- lm(reg_dataset, UNEM ~ HGRAD + ROLL)
lmfit <- lm(data = reg_dataset, UNEM ~ HGRAD + ROLL)
View(lmfit)
?predict()
predict(lmfit, c(90000, 0.07))
predict(lmfit, r(90000, 0.07))
predict(lmfit, data.frame(90000, 0.07))
predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07))
lmfit <- lm(data = reg_dataset, ROLL ~ HGRAD + UNEM)
predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07))
lm_predicted <- predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07))
print('The predicted enrollment is ',predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07)),'if UNEM = 7% and HGRAD = 90000.')
print('The predicted enrollment is ', predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07)[1]),'if UNEM = 7% and HGRAD = 90000.')
lmfit <- lm(data = reg_dataset, ROLL ~ HGRAD + UNEM)
print('The predicted enrollment is ', predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07)[1]),'if UNEM = 7% and HGRAD = 90000.')
predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07))
mfv(EPI) #mode
#--------------------------------------------------------
# Data Analytics 2021 FALL Lab02                         |
# By: Haolin Xiong                                       |
# RCS: xiongh                                            |
# 09/24/2021                                             |
#--------------------------------------------------------
#-------------------------------------------------------
#                        Part 1a                        |
#-------------------------------------------------------
library(modeest)
library(dplyr)
library(ggplot2)
library(gplots)
EPI_dataset <- read.csv('https://aquarius.tw.rpi.edu/html/DA/EPI/EPI_data.csv')
EPI <- as.numeric(EPI_dataset$EPI)
EPI <- na.omit(EPI)
mean(EPI) #mean
mfv(EPI) #mode
median(EPI) #median
fivenum(EPI) # five num summary
DALY <- as.numeric(EPI_dataset$DALY)
DALY <- na.omit(DALY)
mean(DALY) #mean
mfv(DALY) #mode
median(DALY) #median
fivenum(DALY) # five num summary
#Generate frequency histograms for EPI and DALY
stem(EPI)
hist(EPI,seq(30,95,1.0),prob=TRUE)
#lines(density(EPI,na.rm=TRUE,bw=1))
lines(density(EPI,na.rm=TRUE,bw='SJ'))
stem(DALY)
hist(DALY,seq(0,100,1.0),prob=TRUE)
#lines(density(DALY,na.rm=TRUE,bw=1))
lines(density(DALY,na.rm=TRUE,bw='SJ'))
#Generate the boxplot
#?boxplot
boxplot(EPI_dataset$ENVHEALTH,EPI_dataset$ECOSYSTEM,names = c('ENVHEALTH', 'ECOSYSTEM'))
#Generate the Q-Q plot
#?qqplot
qqplot(EPI_dataset$ENVHEALTH,EPI_dataset$ECOSYSTEM,xlab = 'ENVHEALTH', ylab = 'ECOSYSTEM')
#-------------------------------------------------------
#                        Part 1b                        |
#-------------------------------------------------------
#Find the most important feature by region
table(EPI_dataset$EPI_regions)
EPI_SSA <- filter(EPI_dataset,EPI_regions == 'Sub-Saharan Africa')
EPI_SSA <- EPI_dataset[, unlist(lapply(EPI_SSA, is.numeric))]
EPI_SSA<-na.omit(EPI_SSA)
EPI_cor <- data.frame(cor(EPI_SSA,EPI_SSA$EPI))
EPI_cor <- na.omit(EPI_cor)
EPI_cor <- cbind(newColName = rownames(EPI_cor), EPI_cor)
rownames(EPI_cor) <- 1:nrow(EPI_cor)
colnames(EPI_cor) <- c('variable name','correlation_with_EPI')
EPI_cor <- EPI_cor %>% arrange(correlation_with_EPI)
View(EPI_cor)
#Linear and Least-Square
boxplot(EPI_dataset$ENVHEALTH,EPI_dataset$DALY,EPI_dataset$AIR_H,EPI_dataset$WATER_H,names = c('ENVHEALTH','DALY','AIR_H','WATER_H'))
lmENVH <- lm(ENVHEALTH~DALY+AIR_H+WATER_H, data = EPI_dataset)
lmENVH
summary(lmENVH)
cENVH <- coef(lmENVH)
lmENVH
DALYNEW <- c(seq(5,95,5))
AIR_HNEW <- c(seq(5,95,5))
WATER_HNEW <- c(seq(5,95,5))
NEW <- data.frame(DALYNEW, AIR_HNEW, WATER_HNEW)
colnames(NEW) <- c('DALY','AIR_H','WATER_H')
pENV <- lmENVH %>% predict(NEW, interval = 'prediction')
cENV <- lmENVH %>% predict(NEW, interval = 'confidence')
View(pENV)
#-------------------------------------------------------
#                  Part 2 Exercise.1                    |
#-------------------------------------------------------
reg_dataset <- read.csv('https://aquarius.tw.rpi.edu/html/DA/dataset_multipleRegression.csv')
lmfit <- lm(data = reg_dataset, ROLL ~ HGRAD + UNEM)
predict(lmfit, data.frame(HGRAD = 90000, UNEM = 0.07))
#-------------------------------------------------------
#                  Part 2 Exercise.2                    |
#-------------------------------------------------------
aba_dataset <- read.csv('https://aquarius.tw.rpi.edu/html/DA/abalone.csv')
#-------------------------------------------------------
#                  Part 2 Exercise.3                    |
#-------------------------------------------------------
mfv(EPI) #mode
#-------------------------------------------------------
#                        Part 1a                        |
#-------------------------------------------------------
library(modeest)
#-------------------------------------------------------
#                        Part 1a                        |
#-------------------------------------------------------
library(genefilter)
#-------------------------------------------------------
#                        Part 1a                        |
#-------------------------------------------------------
intall.package(genefilter)
install.packages("genefilter")
install.packages("genefilter")
install.packages("statip")
#-------------------------------------------------------
#                        Part 1a                        |
#-------------------------------------------------------
library(statip)
mfv(EPI) #mode
abalone <- read.csv('https://aquarius.tw.rpi.edu/html/DA/abalone.csv')
colnames(abalone) <- c("sex", "length", 'diameter', 'height', 'whole_weight', 'shucked_wieght', 'viscera_wieght', 'shell_weight',
'rings' )
abalone$rings <- as.numeric(abalone$rings)
abalone$rings <- cut(abalone$rings, br=c(-1,8,11,35), labels = c("young", 'adult', 'old'))
abalone$rings <- as.factor(abalone$rings)
aba <- abalone
aba$sex <- NULL
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
aba[1:7] <- as.data.frame(lapply(aba[1:7], normalize))
summary(aba$shucked_wieght)
# After Normalization, each variable has a min of 0 and a max of 1.
# in other words, values are in the range from 0 to 1.
# We'll now split the data into training and testing sets.
ind <- sample(2, nrow(aba), replace=TRUE, prob=c(0.7, 0.3))
KNNtrain <- aba[ind==1,]
KNNtest <- aba[ind==2,]
sqrt(2918)
KNNpred
sqrt(len(aba))
sqrt(nrow(aba))
sqrt(nrow(KNNtrain))
ind <- sample(2, nrow(aba), replace=TRUE, prob=c(0.7, 0.3))
KNNtrain <- aba[ind==1,]
KNNtest <- aba[ind==2,]
k <- sqrt(nrow(KNNtrain))
help("knn") # Read the knn documentation on RStudio.
KNNpred <- knn(train = KNNtrain[1:7], test = KNNtest[1:7], cl = KNNtrain$rings, k = k)
KNNpred
table(KNNpred)
library(class)
KNNtrain <- aba[ind==1,]
KNNtest <- aba[ind==2,]
k <- sqrt(nrow(KNNtrain))
help("knn") # Read the knn documentation on RStudio.
KNNpred <- knn(train = KNNtrain[1:7], test = KNNtest[1:7], cl = KNNtrain$rings, k = k)
KNNpred
table(KNNpred)
ind <- sample(2, nrow(aba), replace=TRUE, prob=c(0.7, 0.3))
KNNtrain <- aba[ind==1,]
KNNtest <- aba[ind==2,]
k <- sqrt(nrow(KNNtrain))
help("knn") # Read the knn documentation on RStudio.
KNNpred <- knn(train = KNNtrain[1:7], test = KNNtest[1:7], cl = KNNtrain$rings, k = k)
KNNpred
table(KNNpred)
cENVH <- coef(lmENVH)
cENVH
summary(lmENVH)
hist(pexp(1000, 1), col = "skyblue")
hist(pexp(, 1), col = "skyblue")
hist(pexp(10000, 1), col = "skyblue")
hist(pexp(1), col = "skyblue")
hist(pexp(-1), col = "skyblue")
hist(exp(-1), col = "skyblue")
hist(exp(10000), col = "skyblue")
hist(dexp(10000), col = "skyblue")
x <- seq(from = 0, to = 250, by = 1)
hist(dexp(x,1), col = "skyblue")
plot(x, dexp(x, 2), type = "l",ylab = "", lwd = 2, col = "red")
x <- seq(from = 0, to = 250, by = 1)
x <- seq(from = 0, to = 100, by = 1)
plot(x, dexp(x, 1/610), type = "l",ylab = "", lwd = 2, col = "red")
x <- seq(from = 0, to = 10000, by = 1)
plot(x, dexp(x, 1/610), type = "l",ylab = "", lwd = 2, col = "red")
?sample
sample(x, 10000, prob = dexp(x, 1/610), replace = T)
first_failed <- sample(x, 10000, prob = dexp(x, 1/610), replace = T)
second_failed <- sample(x, 10000, prob = dexp(x, 1/610), replace = T)
first_failed <- sample(x, 10000, prob = dexp(x, 1/610), replace = T)
second_failed <- sample(x, 10000, prob = dexp(x, 1/610), replace = T)
x <- seq(from = 0, to = 10000, by = 1)
first_failed <- sample(x, 10000, prob = dexp(x, 1/610), replace = T)
second_failed <- sample(x, 10000, prob = dexp(x, 1/610), replace = T)
actual_failed <- c(10000)
actual_failed <- c()
actual_failed[1] <- 100
actual_failed[2] <- 100
actual_failed <- c()
actual_failed[2] <- 100
actual_failed <- c()
for (i in range(10000)) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed[i] <- min(num1,num2)
}
min(num1,num2)
i
actual_failed[i]
View(actual_failed)
?append
actual_failed <- c()
for (i in range(10000)) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
?range
for (i in c(1,10000)) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
actual_failed <- c()
for (i in c(1,10000)) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
actual_failed <- c()
for (i in x) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
x <- seq(from = 1, to = 10000, by = 1)
actual_failed <- c()
for (i in x) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
RIN <- 610
trial <- function(){
actual_failed <- c()
first_failed <- sample(x, 10000, prob = dexp(x, 1/RIN), replace = T)
second_failed <- sample(x, 10000, prob = dexp(x, 1/RIN), replace = T)
for (i in x) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
}
x <- seq(from = 1, to = 10000, by = 1)
plot(x, dexp(x, 1/610), type = "l",ylab = "", lwd = 2, col = "red")
plot(x, pexp(x, 1/610), type = "l",ylab = "", lwd = 2, col = "red")
x <- seq(from = 1, to = 10000, by = 1)
plot(x, pexp(x, 1/610), type = "l",ylab = "", lwd = 2, col = "red")
RIN <- 610
actual_failed <- c()
first_failed <- sample(x, 10000, prob = pexp(x, 1/RIN), replace = T)
second_failed <- sample(x, 10000, prob = pexp(x, 1/RIN), replace = T)
for (i in x) {
num1 <- first_failed[i]
num2 <- second_failed[i]
actual_failed <- append(actual_failed,min(num1,num2))
}
# ISLR: Introduction to Statistical Learning with R (textbook that we use in this class)
# Validation set example with Auto dataset.
library(ISLR)
library(MASS)
library(boot)
set.seed(1)
# Read the cv.glm documentation
??cv.glm
# read the documentation for sample() function
help("sample")
train = sample(392,196)
# We use the subset option in the lm() function to fit a linear regression using,
# only the observations corresponding to the training set.
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
# Now we use predict() function to estimate the response for all 392 observations,
# and we use the mean() function to calculate the MSE of the 196 observations in the
# validation set. Note that the -train selects only the observations that are not in,
# the training set.
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Therefore, the estimated test MSE for the linear regression fit is 26.14
# We can use the poly() function to estimate test error for the quadratic and cubic regression.
# Quadratic regression line
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train) # Quadratic
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# Cubic regression line
lm.fit3 <- lm(mpg~poly(horsepower,3), data = Auto, subset = train) # Cubic
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
# The error rates are: 19.82 for quadratics and 19.78 for cubic
# If we choose different training set instead, then we will obtain somewhat different errors,
# on the validation set.
set.seed(2)
train = sample(392,196)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# the error rate is 23.29
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train) # Quadratic
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# the error rate is 18.90
lm.fit3 <- lm(mpg~poly(horsepower,3), data = Auto, subset = train) # Cubic
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
# the error rate is 19.25
# Using this split of the observations into a training set and validation set,
# we find that the validation set error rates for the models with linear, quadratic,
# and cubic terms are 23.29, 18.90 and 19.25 respectively.
# The model that predict mpg using a quadratic function of horsepower performs better,
# than a models that only involves only a linear function of horsepower, and there is a,
# little evidence in favor of a model that uses a cubic function of horsepower.
install.packages("ISLR")
install.packages("MASS")
install.packages("boot")
install.packages("MASS")
# ISLR: Introduction to Statistical Learning with R (textbook that we use in this class)
# Validation set example with Auto dataset.
library(ISLR)
library(MASS)
library(boot)
set.seed(1)
# Read the cv.glm documentation
??cv.glm
# read the documentation for sample() function
help("sample")
train = sample(392,196)
# We use the subset option in the lm() function to fit a linear regression using,
# only the observations corresponding to the training set.
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
# Now we use predict() function to estimate the response for all 392 observations,
# and we use the mean() function to calculate the MSE of the 196 observations in the
# validation set. Note that the -train selects only the observations that are not in,
# the training set.
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Therefore, the estimated test MSE for the linear regression fit is 26.14
# We can use the poly() function to estimate test error for the quadratic and cubic regression.
# Quadratic regression line
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train) # Quadratic
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# Cubic regression line
lm.fit3 <- lm(mpg~poly(horsepower,3), data = Auto, subset = train) # Cubic
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
# The error rates are: 19.82 for quadratics and 19.78 for cubic
# If we choose different training set instead, then we will obtain somewhat different errors,
# on the validation set.
set.seed(2)
train = sample(392,196)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# the error rate is 23.29
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train) # Quadratic
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# the error rate is 18.90
lm.fit3 <- lm(mpg~poly(horsepower,3), data = Auto, subset = train) # Cubic
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
# the error rate is 19.25
# Using this split of the observations into a training set and validation set,
# we find that the validation set error rates for the models with linear, quadratic,
# and cubic terms are 23.29, 18.90 and 19.25 respectively.
# The model that predict mpg using a quadratic function of horsepower performs better,
# than a models that only involves only a linear function of horsepower, and there is a,
# little evidence in favor of a model that uses a cubic function of horsepower.
??cv.glm
set.seed(17)
help("rep") # read the documentation for the rep() function in R.
cv.error.10 = rep(0,10) # read documentation, help("rep")
for(i in 1:10){
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] = cv.glm(Auto,glm.fit, K=10) $delta[1]
}
cv.error.10
# Notice the computation time is much shorter than LOOCV! :),
# This depends on your laptop performance :)
# We still see little evidence that using cubic or higher-order polynomials terms,
# leads to lower test error than simply using a quadratics fit.
install.packages("titanic")
install.packages("titanic")
data("titanic")
data("Titanic")
df <- data("Titanic")
View(df)
df <- data("Titanic").data
df <- data("Titanic"
df <- data("Titanic")
df <- data("Titanic")
View(df)
data("Titanic")
Summary(Titanic)
summary(Titanic)
Titanic
View(Titanic)
titan_rpart <- rpart(Freq ~ Class + Sex + Age + Survived, data = Titanic)
plot(titan_rpart) # try some different plot options
text(titan_rpart) # try some different text options
titan_survied <- filter(Titanic,Titanic$Survived == 'Yes')
?filter
library(diplyr)
library(dplyr)
titan_survied <- filter(Titanic,Titanic$Survived == 'Yes')
Titanic
Titanic$Survived
Titanic.Survived
Titanic$Survived
titanic
install.packages("titanic")
library(titanic)
titanic
titanic.data
?titanic
titanic.train
attach(titanic)
titanic
data
data(Titanic)
setwd("D:/GitHub/DataAnalytics2021_Haolin_Xiong/Labs/Lab5/titanic")
titanic_df <- read.csv('train.csv')
View(titanic_df)
titan_rpart <- rpart(Survived ~ Pclass + Sex + Age + Fare, data = titanic_df)
plot(titan_rpart) # try some different plot options
text(titan_rpart) # try some different text options
?pairs
titan_ctree <- ctree(Survived ~ Pclass + Sex + Age + Fare, data = titanic_df)
require(party)
library(party)
titan_ctree <- ctree(Survived ~ Pclass + Sex + Age + Fare, data = titanic_df)
plot(swiss_ctree)
plot(titan_ctree)
hc <- hclust(titanic_df, "ave")
titanic_df_dropna <- titanic_df.dropna()
titanic_df_dropna <- na.omit(titanic_df)
hc <- hclust(titanic_df_dropna, "ave")
plot(hc)
titanic_df_dropna
View(titanic_df_dropna)
titanic_df_hc <- titanic_df[,[2,3,5,6,10]]
titanic_df_hc <- titanic_df[,c(2,3,5,6,10)]
titanic_df_hc
hc <- hclust(titanic_df_hc, "ave")
